{
 "cells": [
{
 "cell_type": "code",
 "execution_count": null,
 "id": "5c47ac9e",
 "metadata": {},
 "outputs": [],
 "source": [
  "\"\"\"\n",
  "Description:\n",
  "An open-source document-based Q&A assistant using LangChain, FAISS, HuggingFace Embeddings, and TinyLLaMA (1.1B) LLM.\n",
  "Supports PDF and TXT files with chunking, embedding, and semantic retrieval.\n",
  "\n",
  "Features:\n",
  "- Load PDF/TXT documents\n",
  "- Chunk with overlap\n",
  "- Embed using all-MiniLM-L6-v2\n",
  "- Store in FAISS vector store\n",
  "- Query using TinyLLaMA model\n",
  "\"\"\"\n"
 ]
},
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Make sure to set your OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize OpenAI Chat Model via LangChain\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "def listen_to_voice():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something...\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing speech...\")\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"You said: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio.\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results; {e}\")\n",
    "        return None\n",
    "\n",
    "def speak(text):\n",
    "    print(f\"Responding: {text}\")\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"response.mp3\")\n",
    "    playsound(\"response.mp3\")\n",
    "    os.remove(\"response.mp3\")\n",
    "\n",
    "def chat_with_ai(prompt):\n",
    "    response = chat([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def main():\n",
    "    print(\"Voice Assistant Ready (Say 'exit' to quit)\")\n",
    "    while True:\n",
    "        prompt = listen_to_voice()\n",
    "        if prompt is None:\n",
    "            continue\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "        reply = chat_with_ai(prompt)\n",
    "        speak(reply)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efea8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Javascript, display\n",
    "from google.colab import output\n",
    "\n",
    "record_js = \"\"\"\n",
    "const record_js = `\n",
    "const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
    "\n",
    "var record = async () => {\n",
    "  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "  const recorder = new MediaRecorder(stream, {mimeType: 'audio/wav'}); // Specify wav mimeType\n",
    "  let data = [];\n",
    "\n",
    "  recorder.ondataavailable = event => data.push(event.data);\n",
    "  recorder.start();\n",
    "\n",
    "  await sleep(4000);  // record for 4 seconds\n",
    "  recorder.stop();\n",
    "\n",
    "  await new Promise(resolve => recorder.onstop = resolve);\n",
    "  const blob = new Blob(data, { type: 'audio/wav' });  // Ensure it's saved as wav\n",
    "  const arrayBuffer = await blob.arrayBuffer();\n",
    "  const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));\n",
    "\n",
    "  google.colab.kernel.invokeFunction('notebook.audio_callback', [base64], {});\n",
    "};\n",
    "\n",
    "record();\n",
    "`;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def save_audio(b64_data):\n",
    "    with open(\"recorded.wav\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(b64_data))\n",
    "\n",
    "output.register_callback('notebook.audio_callback', save_audio)\n",
    "display(Javascript(record_js))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55feeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"recorded.wav\")\n",
    "text = result[\"text\"]\n",
    "print(\"You said:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af099905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from IPython.display import Audio\n",
    "\n",
    "def convert_to_pcm_wav(input_file, output_file=\"converted.wav\"):\n",
    "    print(\"Converting MP3 to WAV...\")\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "    print(\"Conversion done.\")\n",
    "    return output_file\n",
    "\n",
    "def listen_to_voice(file_path=\"recorded.mp3\"):\n",
    "    wav_path = convert_to_pcm_wav(file_path)\n",
    "    Audio(wav_path)  # Play to confirm sound\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(wav_path) as source:\n",
    "        print(\"Processing audio...\")\n",
    "        audio = recognizer.record(source)  # or duration=5\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing...\")\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"You said: {text}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "text = listen_to_voice(\"recorded.mp3\")\n",
    "print(\"Final Transcription:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from IPython.display import Audio as ColabAudio\n",
    "\n",
    "# Load a lightweight LLM (change to bigger if using GPU: 'tiiuae/falcon-7b-instruct' or 'mistralai/Mistral-7B-Instruct-v0.1')\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "def convert_to_pcm_wav(input_file, output_file=\"converted.wav\"):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "    return output_file\n",
    "\n",
    "def listen_to_voice(file_path=\"recorded.mp3\"):\n",
    "    wav_path = convert_to_pcm_wav(file_path)\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(wav_path) as source:\n",
    "        print(\"Processing audio...\")\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing...\")\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"You said: {text}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "def speak(text):\n",
    "    print(f\"Responding: {text}\")\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"response.mp3\")\n",
    "    return ColabAudio(\"response.mp3\", autoplay=True)\n",
    "\n",
    "def chat_with_llm(prompt):\n",
    "    print(\"Generating response...\")\n",
    "    result = generator(prompt, max_length=100)[0]['generated_text']\n",
    "    return result.strip()\n",
    "\n",
    "def main():\n",
    "    print(\"Voice Assistant Ready (Say 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_input = listen_to_voice(\"recorded.mp3\")  # Upload this each time\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        reply = chat_with_llm(user_input)\n",
    "        speak(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = listen_to_voice(\"recorded.mp3\")\n",
    "reply = chat_with_llm(text)\n",
    "speak(reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
